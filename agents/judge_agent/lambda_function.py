import json
import logging
import os
import re
from datetime import datetime
from typing import Any, Dict, List

logger = logging.getLogger()
logger.setLevel(logging.INFO)


class JudgeAgent:
    """
    è©•ä¾¡çµæœã®åˆ¤å®šã¨æ¬¡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®æ±ºå®šã‚’è¡Œã†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
    """

    def __init__(self):
        self.logger = logger
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé–¾å€¤ã®èª­ã¿è¾¼ã¿ï¼ˆç’°å¢ƒå¤‰æ•°ã¾ãŸã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ï¼‰
        self.default_thresholds = self._load_default_thresholds()

    def _load_default_thresholds(self) -> Dict[str, Dict[str, float]]:
        """
        ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé–¾å€¤ã‚’ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯S3è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿

        ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å€¤ã‚’ä½¿ç”¨
        """
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ã®èª­ã¿è¾¼ã¿ï¼ˆJSONå½¢å¼ï¼‰
        thresholds_json = os.getenv("DEFAULT_THRESHOLDS")

        if thresholds_json:
            try:
                return json.loads(thresholds_json)
            except json.JSONDecodeError:
                self.logger.warning(
                    "Failed to parse DEFAULT_THRESHOLDS from env, " "using hardcoded defaults"
                )

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å€¤
        return {
            "classification": {
                "min_accuracy": 0.85,
                "min_precision": 0.80,
                "min_recall": 0.80,
                "min_f1_score": 0.80,
                "min_auc_roc": 0.85,
            },
            "regression": {"max_rmse": 10.0, "max_mae": 5.0, "min_r2_score": 0.80},
            "clustering": {"min_silhouette_score": 0.50},
            "reinforcement_learning": {"min_avg_reward": 100, "min_success_rate": 0.70},
        }

    def _validate_github_url(self, url: str) -> bool:
        """
        GitHub Issueã®URLæ¤œè¨¼ï¼ˆURLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–ï¼‰

        Args:
            url: æ¤œè¨¼ã™ã‚‹URL

        Returns:
            True: æœ‰åŠ¹ãªGitHub Issue URL
            False: ç„¡åŠ¹ãªURL
        """
        if not url:
            return True  # URLãŒç©ºã®å ´åˆã¯æ¤œè¨¼ã‚¹ã‚­ãƒƒãƒ—

        pattern = r"^https://github\.com/[\w-]+/[\w-]+/issues/\d+$"
        return re.match(pattern, url) is not None

    def judge_model(self, event: Dict[str, Any]) -> Dict[str, Any]:
        """
        ãƒ¢ãƒ‡ãƒ«è©•ä¾¡çµæœã‚’åˆ¤å®šã—ã€æ¬¡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ±ºå®š

        Args:
            event: Step Functionsã‹ã‚‰ã®å…¥åŠ›ãƒ‡ãƒ¼ã‚¿

        Returns:
            åˆ¤å®šçµæœã¨æ¬¡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        """
        training_id = event["training_id"]
        task_type = event["task_type"]
        evaluation_results = event["evaluation_results"]
        acceptance_criteria = event["acceptance_criteria"]
        retry_count = event.get("retry_count", 0)
        max_retries = event.get("max_retries", 3)

        # GitHub Issue URLæ¤œè¨¼ï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–ï¼‰
        github_issue_url = event.get("github_issue_url", "")
        if github_issue_url and not self._validate_github_url(github_issue_url):
            raise ValueError(f"Invalid GitHub Issue URL: {github_issue_url}")

        self.logger.info(f"Judging model: {training_id}, task_type: {task_type}")

        # åˆ¤å®šå®Ÿè¡Œ
        is_acceptable, passed_criteria, failed_criteria = self._evaluate_criteria(
            task_type, evaluation_results, acceptance_criteria
        )

        # æ¬¡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ±ºå®š
        next_action = self._determine_next_action(is_acceptable, retry_count, max_retries)

        # çµæœæ§‹ç¯‰
        result = {
            "training_id": training_id,
            "judgment": {
                "is_acceptable": is_acceptable,
                "passed_criteria": passed_criteria,
                "failed_criteria": failed_criteria,
            },
            "next_action": next_action,
            "retry_count": retry_count if not is_acceptable else 0,
            "timestamp": datetime.utcnow().isoformat() + "Z",
        }

        # ã‚ªãƒšãƒ¬ãƒ¼ã‚¿é€šçŸ¥ãŒå¿…è¦ãªå ´åˆ
        if next_action["notify_operator"]:
            result["operator_notification"] = self._create_notification_message(
                training_id, failed_criteria, retry_count, max_retries, event
            )

        self.logger.info(f"Judgment result: {json.dumps(result, indent=2)}")
        return result

    def _evaluate_criteria(
        self,
        task_type: str,
        evaluation_results: Dict[str, float],
        acceptance_criteria: Dict[str, float],
    ) -> tuple[bool, List[str], List[str]]:
        """
        å—å…¥åŸºæº–ã®è©•ä¾¡

        Returns:
            (is_acceptable, passed_criteria, failed_criteria)
        """
        passed = []
        failed = []

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé–¾å€¤ã‚’å–å¾—ï¼ˆç’°å¢ƒå¤‰æ•°ã¾ãŸã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ï¼‰
        default_thresholds = self.default_thresholds.get(task_type, {})

        if task_type == "classification":
            criteria_map = {
                "accuracy": ("min_accuracy", default_thresholds.get("min_accuracy", 0.85), ">="),
                "precision": ("min_precision", default_thresholds.get("min_precision", 0.80), ">="),
                "recall": ("min_recall", default_thresholds.get("min_recall", 0.80), ">="),
                "f1_score": ("min_f1_score", default_thresholds.get("min_f1_score", 0.80), ">="),
                "auc_roc": ("min_auc_roc", default_thresholds.get("min_auc_roc", 0.85), ">="),
            }
        elif task_type == "regression":
            criteria_map = {
                "rmse": ("max_rmse", default_thresholds.get("max_rmse", 10.0), "<="),
                "mae": ("max_mae", default_thresholds.get("max_mae", 5.0), "<="),
                "r2_score": ("min_r2_score", default_thresholds.get("min_r2_score", 0.80), ">="),
            }
        elif task_type == "clustering":
            criteria_map = {
                "silhouette_score": (
                    "min_silhouette_score",
                    default_thresholds.get("min_silhouette_score", 0.50),
                    ">=",
                )
            }
        elif task_type == "reinforcement_learning":
            criteria_map = {
                "avg_reward": (
                    "min_avg_reward",
                    default_thresholds.get("min_avg_reward", 100),
                    ">=",
                ),
                "success_rate": (
                    "min_success_rate",
                    default_thresholds.get("min_success_rate", 0.70),
                    ">=",
                ),
            }
        else:
            raise ValueError(f"Unknown task_type: {task_type}")

        for metric_name, (criteria_key, default_threshold, operator) in criteria_map.items():
            if metric_name not in evaluation_results:
                continue

            actual_value = evaluation_results[metric_name]
            threshold = acceptance_criteria.get(criteria_key, default_threshold)

            if operator == ">=":
                is_passed = actual_value >= threshold
            else:  # "<="
                is_passed = actual_value <= threshold

            criterion_text = f"{metric_name} {operator} {threshold} (actual: {actual_value:.4f})"

            if is_passed:
                passed.append(criterion_text)
            else:
                failed.append(criterion_text)

        is_acceptable = len(failed) == 0
        return is_acceptable, passed, failed

    def _determine_next_action(
        self, is_acceptable: bool, retry_count: int, max_retries: int
    ) -> Dict[str, Any]:
        """
        æ¬¡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®æ±ºå®š
        """
        if is_acceptable:
            return {
                "action": "deploy",
                "reason": "Model meets all acceptance criteria",
                "notify_operator": False,
            }

        if retry_count < max_retries:
            return {
                "action": "retry",
                "reason": f"Model does not meet criteria. Retry {retry_count + 1}/{max_retries}",
                "notify_operator": True,
            }

        return {
            "action": "abort",
            "reason": f"Model failed after {max_retries} retries. Manual intervention required.",
            "notify_operator": True,
        }

    def _create_notification_message(
        self,
        training_id: str,
        failed_criteria: List[str],
        retry_count: int,
        max_retries: int,
        event: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        ã‚ªãƒšãƒ¬ãƒ¼ã‚¿å‘ã‘é€šçŸ¥ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ç”Ÿæˆ
        """
        message_lines = [
            "ğŸš¨ **ãƒ¢ãƒ‡ãƒ«è©•ä¾¡çµæœ: ä¸åˆæ ¼**",
            "",
            f"**Training ID**: `{training_id}`",
            f"**ãƒªãƒˆãƒ©ã‚¤å›æ•°**: {retry_count + 1}/{max_retries}",
            "",
            "**å¤±æ•—ã—ãŸåŸºæº–**:",
        ]

        for criterion in failed_criteria:
            message_lines.append(f"- {criterion}")

        message_lines.extend(
            [
                "",
                "**æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**:",
                "1. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´ï¼ˆå­¦ç¿’ç‡ã€æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç­‰ï¼‰",
                "2. ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã®è¿½åŠ ",
                "3. ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾ç­–ã®å¼·åŒ–ï¼ˆSMOTEã€ã‚¯ãƒ©ã‚¹ã‚¦ã‚§ã‚¤ãƒˆèª¿æ•´ç­‰ï¼‰",
                "4. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®è¦‹ç›´ã—",
                "",
                "GitHub Issueã«ã‚³ãƒ¡ãƒ³ãƒˆã—ã¦èª¿æ•´å†…å®¹ã‚’æŒ‡ç¤ºã—ã¦ãã ã•ã„:",
                f"{event.get('github_issue_url', 'N/A')}",
            ]
        )

        return {
            "message": "\n".join(message_lines),
            "channels": event.get("notification_channels", ["slack", "github"]),
        }


def lambda_handler(event, context):
    """
    Lambdaé–¢æ•°ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
    """
    try:
        judge_agent = JudgeAgent()
        result = judge_agent.judge_model(event)

        return {"statusCode": 200, "body": result}

    except Exception as e:
        logger.error(f"Error in Judge Agent: {str(e)}", exc_info=True)
        return {
            "statusCode": 500,
            "body": {"error": str(e), "training_id": event.get("training_id", "unknown")},
        }
